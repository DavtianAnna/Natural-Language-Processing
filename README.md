# NLP with Disaster Tweets - Kaggle Classification Challenge

This repository contains my solution for the [NLP with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/data) Kaggle competition.  
The objective is to classify tweets into disaster-related (`1`) or not (`0`) â€” a binary text classification task.

---

## ğŸ“ Project Structure

```

project-root/


â”œâ”€â”€ ğŸ“„ Task.ipynb                # Main notebook: full ML pipeline from preprocessing to submission.
â”œâ”€â”€ ğŸ“Š train.csv                 # Training dataset (tweets + target)
â”œâ”€â”€ ğŸ§ª test.csv                  # Test dataset (tweets without target)
â”œâ”€â”€ ğŸ“ sample_submission.csv     # Template for Kaggle submission.
â””â”€â”€ ğŸš€ submission1.csv           # Submission from TfidfVectorizer + XGBClassifier
â””â”€â”€ ğŸš€ submission2.csv           # Submission from TfidfVectorizer + LogisticRegression
â””â”€â”€ ğŸ“œ README.md                 # Project documentation.

```


---

## ğŸ’» Technologies Used

- **Language**: Python 3.x  
- **Environment**: Jupyter Notebook


## ğŸ“¦ Libraries

- `pandas`, `numpy` â€“ Data manipulation  
- `matplotlib`, `seaborn` â€“ Visualization  
- `scikit-learn` â€“ Preprocessing & modeling (`LogisticRegression`, `LabelEncoder`, `classification_report` and `TfidfVectorizer` - for text vectorization)  
- `GridSearchCV` â€“ Hyperparameter tuning  
- `xgboost` â€“ Gradient boosting (`XGBClassifier`)

---

## ğŸ“Š Dataset Description

Competition: [NLP with Disaster Tweets â€“ Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/data)

- `train.csv`:  Training data including id, keyword, location, text, and the target label.
- `test.csv`: Test data with the same columns as the training data, but without the target column.
- `sample_submission.csv`: Format required for submitting predictions

---

## ğŸ” Workflow Summary

The `Task.ipynb` notebook implements the complete ML pipeline:

### 1. ğŸ“¥ Data Loading & Preprocessing
- Loaded `train.csv` and `test.csv`.
- EDA ` Check class distribution, look for missing values, explore some tweets, visualize word frequencies.
- Feature Engineering` Convert text into numerical features using TfidfVectorizer.


### 2. ğŸ§  Modeling & Tuning

Three classification models were built and tuned using `TfidfVectorizer`:

| Model                  | Method                  | Output File       |
|------------------------|-------------------------|-------------------|
| `XGBClassifier`        | Pipeline + GridSearchCV | `submission1.csv` |
| `LogisticRegression`   | Pipeline + GridSearchCV | `submission2.csv` |


### 3. ğŸ“¤ Prediction & Submission

Final predictions for each model were saved:

- `submission1.csv` â†’ XGBClassifier
- `submission2.csv` â†’ LogisticRegression

---

## ğŸ“ˆ Results Summary

| Model                  | Accuracy |    Output File    |
|------------------------|----------|-------------------|
| XGBClassifier          | 0.87     | `submission1.csv` |
| LogisticRegression     | 0.80     | `submission2.csv` |

---

## âš™ï¸ Installation

To install all required libraries:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost




